# Base LLaMA Configuration
data:
  data_dir: "/mnt/home/lserrano/data/"
  dataset_name: "advection"
  token_dataset_path: "/mnt/home/lserrano/zebra/tokens/"
  tokenizer_path: "/mnt/home/lserrano/zebra/outputs/"
  sub_t: 10
  slice_size: 10
  num_context_trajectories: 1

# Model parameters
model:
  vocab_size:   # Size of the vocabulary (should match tokenizer)
  hidden_size: 256  # Hidden size of the model
  num_hidden_layers: 8  # Number of transformer layers
  num_attention_heads: 8  # Number of attention heads
  intermediate_size: 1024  # Size of the intermediate layer
  max_position_embeddings: 8192  # Maximum sequence length
  rms_norm_eps: 1e-6  # RMS norm epsilon
  num_dimensions: 1
  max_length: 8192
  bos_token_id: 
  eos_token_id: 
  context_token_id: 
  input_token_id: 
  target_token_id: 
  bot_token_id: 
  eot_token_id: 
  pad_token_id:  

training:
  learning_rate: 1e-4
  weight_decay: 1e-4
  batch_size: 64
  max_steps: 100000
  warmup_steps: 1000
  seed: 42
  num_workers: 4
  accelerator: "auto"
  devices: 1
  strategy: "auto"
  precision: "32"
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  tokenize_on_the_fly: False

logging:
  project: "zebra"
  entity: null  # Set this to your wandb username
  tags: ["llama", "1d"]
  save_every_n_steps: 5000
  checkpoint_every_n_steps: 5000 
  output_dir: "~/zebra/outputs"

# Hugging Face parameters
huggingface:
  push_to_hub: false  # Whether to push the model to the Hugging Face Hub
  repo_name: "your-username/zebra-llama"  # Repository name on the Hugging Face Hub
  token: null  # Your Hugging Face token
  private: false  # Whether the repository should be private
  commit_message: "Upload model"  # Commit message for the push
  model_card: true  # Whether to create a model card
  model_card_template: "zebra_model_card.md"  # Template for the model card 