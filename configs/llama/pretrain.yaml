# LLaMA Pretraining Configuration

# Inherit from base config
defaults:
  - base

# Model parameters
model:
  # Override base model parameters if needed
  hidden_size: 2048
  num_hidden_layers: 24
  num_attention_heads: 32

# Training parameters
learning_rate: 5e-5  # Lower learning rate for pretraining
max_steps: 500000  # More steps for pretraining
warmup_steps: 10000
accumulate_grad_batches: 8  # Larger batch size through gradient accumulation

# Data parameters
batch_size: 16  # Smaller batch size for pretraining
sequence_length: 20
num_context_sequences: 5

# Logging parameters
wandb:
  tags: ["llama", "pretrain"]

# Generation parameters
generation:
  max_length: 8192
  num_return_sequences: 1
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  repetition_penalty: 1.0
  do_sample: true

# Checkpointing
checkpoint:
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"
  every_n_train_steps: 1000
  filename: "{step}-{val_loss:.4f}" 